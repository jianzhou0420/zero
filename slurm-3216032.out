Running task 
/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /hpcfs/users/a1946536/zero/2_Train/2025_02_22__15-32_single_try
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type               | Params
---------------------------------------------
0 | model | SimplePolicyPTV3CA | 68.3 M
---------------------------------------------
68.3 M    Trainable params
0         Non-trainable params
68.3 M    Total params
273.342   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
No flash attn
[insert_onto_square_peg]
Total steps: 480000, Warmup steps: 32000
fp16: False
tasks_all: ['insert_onto_square_peg']
max_cache_length: 1800
tasks_all: ['insert_onto_square_peg']
max_cache_length: 1800
Train dataset size: 100, Val dataset size: 4
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  0.53it/s]                                                                           batch_size: 4
/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/25 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/25 [00:00<?, ?it/s] Traceback (most recent call last):
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/trainer_expbase.py", line 349, in <module>
    train(config)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/trainer_expbase.py", line 278, in train
    trainer.fit(trainer_model, datamodule=data_module)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/models/lotus/optim/adamw.py", line 63, in step
    loss = closure()
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/trainer_expbase.py", line 80, in training_step
    losses = self.model(batch, is_train=True)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/models/lotus/model_expbase.py", line 244, in forward
    point_outs = self.ptv3_model(ptv3_batch, return_dec_layers=True)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/models/lotus/PointTransformerV3/model_ca.py", line 439, in forward
    point = self.embedding(point)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/models/lotus/PointTransformerV3/model.py", line 867, in forward
    point = self.stem(point)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratchdata1/users/a1946536/zero/zero/expBaseV5/models/lotus/PointTransformerV3/model.py", line 250, in forward
    input.feat = module(input.feat)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 735, in forward
    world_size = torch.distributed.get_world_size(process_group)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1067, in get_world_size
    return _get_group_size(group)
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 453, in _get_group_size
    default_pg = _get_default_group()
  File "/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 584, in _get_default_group
    raise RuntimeError(
RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.
Epoch 0:   0%|          | 0/25 [00:01<?, ?it/s]
