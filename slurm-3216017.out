Running task 
/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

Missing logger folder: /hpcfs/users/a1946536/zero/2_Train/2025_02_22__15-29_single_try
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type               | Params
---------------------------------------------
0 | model | SimplePolicyPTV3CA | 68.3 M
---------------------------------------------
68.3 M    Trainable params
0         Non-trainable params
68.3 M    Total params
273.342   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
No flash attn
[insert_onto_square_peg]
Total steps: 480000, Warmup steps: 32000
fp16: False
tasks_all: ['insert_onto_square_peg']
max_cache_length: 1800
tasks_all: ['insert_onto_square_peg']
max_cache_length: 1800
Train dataset size: 100, Val dataset size: 4
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00,  0.08it/s]                                                                           batch_size: 4
/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/a1946536/.conda/envs/zero/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/25 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/25 [00:00<?, ?it/s] Epoch 0:   4%|â–         | 1/25 [00:06<02:42,  0.15it/s]Epoch 0:   4%|â–         | 1/25 [00:06<02:42,  0.15it/s, v_num=0, train_loss_step=19.40]Epoch 0:   8%|â–Š         | 2/25 [00:07<01:21,  0.28it/s, v_num=0, train_loss_step=19.40]Epoch 0:   8%|â–Š         | 2/25 [00:07<01:21,  0.28it/s, v_num=0, train_loss_step=19.50]Epoch 0:  12%|â–ˆâ–        | 3/25 [00:07<00:54,  0.40it/s, v_num=0, train_loss_step=19.50]Epoch 0:  12%|â–ˆâ–        | 3/25 [00:07<00:54,  0.40it/s, v_num=0, train_loss_step=19.60]Epoch 0:  16%|â–ˆâ–Œ        | 4/25 [00:07<00:40,  0.52it/s, v_num=0, train_loss_step=19.60]Epoch 0:  16%|â–ˆâ–Œ        | 4/25 [00:07<00:40,  0.52it/s, v_num=0, train_loss_step=19.50]Epoch 0:  20%|â–ˆâ–ˆ        | 5/25 [00:08<00:32,  0.62it/s, v_num=0, train_loss_step=19.50]Epoch 0:  20%|â–ˆâ–ˆ        | 5/25 [00:08<00:32,  0.62it/s, v_num=0, train_loss_step=19.60]Epoch 0:  24%|â–ˆâ–ˆâ–       | 6/25 [00:08<00:26,  0.72it/s, v_num=0, train_loss_step=19.60]Epoch 0:  24%|â–ˆâ–ˆâ–       | 6/25 [00:08<00:26,  0.72it/s, v_num=0, train_loss_step=19.50]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:22,  0.80it/s, v_num=0, train_loss_step=19.50]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:22,  0.80it/s, v_num=0, train_loss_step=19.60]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:09<00:19,  0.89it/s, v_num=0, train_loss_step=19.60]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:09<00:19,  0.89it/s, v_num=0, train_loss_step=19.50]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:09<00:16,  0.96it/s, v_num=0, train_loss_step=19.50]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:09<00:16,  0.96it/s, v_num=0, train_loss_step=19.50]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:09<00:14,  1.03it/s, v_num=0, train_loss_step=19.50]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:09<00:14,  1.03it/s, v_num=0, train_loss_step=19.50]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:10<00:12,  1.10it/s, v_num=0, train_loss_step=19.50]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:10<00:12,  1.10it/s, v_num=0, train_loss_step=19.50]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:10<00:11,  1.16it/s, v_num=0, train_loss_step=19.50]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:10<00:11,  1.16it/s, v_num=0, train_loss_step=19.50]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:10<00:09,  1.22it/s, v_num=0, train_loss_step=19.50]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:10<00:09,  1.22it/s, v_num=0, train_loss_step=19.40]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:10<00:08,  1.28it/s, v_num=0, train_loss_step=19.40]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:10<00:08,  1.28it/s, v_num=0, train_loss_step=19.40]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:11<00:07,  1.33it/s, v_num=0, train_loss_step=19.40]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:11<00:07,  1.33it/s, v_num=0, train_loss_step=19.50]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:11<00:06,  1.38it/s, v_num=0, train_loss_step=19.50]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:11<00:06,  1.38it/s, v_num=0, train_loss_step=19.50]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:11<00:05,  1.43it/s, v_num=0, train_loss_step=19.50]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:11<00:05,  1.43it/s, v_num=0, train_loss_step=19.40]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:12<00:04,  1.48it/s, v_num=0, train_loss_step=19.40]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:12<00:04,  1.48it/s, v_num=0, train_loss_step=19.40]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:12<00:03,  1.52it/s, v_num=0, train_loss_step=19.40]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:12<00:03,  1.52it/s, v_num=0, train_loss_step=19.50]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:12<00:03,  1.56it/s, v_num=0, train_loss_step=19.50]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:12<00:03,  1.56it/s, v_num=0, train_loss_step=19.50]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:13<00:02,  1.60it/s, v_num=0, train_loss_step=19.50]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:13<00:02,  1.60it/s, v_num=0, train_loss_step=19.40]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:13<00:01,  1.64it/s, v_num=0, train_loss_step=19.40]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:13<00:01,  1.64it/s, v_num=0, train_loss_step=19.50]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:13<00:01,  1.68it/s, v_num=0, train_loss_step=19.50]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:13<00:01,  1.68it/s, v_num=0, train_loss_step=19.40]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:14<00:00,  1.71it/s, v_num=0, train_loss_step=19.40]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:14<00:00,  1.71it/s, v_num=0, train_loss_step=19.40]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.75it/s, v_num=0, train_loss_step=19.40]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.75it/s, v_num=0, train_loss_step=19.40]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.31it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.67it/s, v_num=0, train_loss_step=19.40]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.67it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]Epoch 0:   0%|          | 0/25 [00:00<?, ?it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]         Epoch 1:   0%|          | 0/25 [00:00<?, ?it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]Epoch 1:   4%|â–         | 1/25 [00:00<00:21,  1.11it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]Epoch 1:   4%|â–         | 1/25 [00:00<00:21,  1.11it/s, v_num=0, train_loss_step=19.50, train_loss_epoch=19.50]Epoch 1:   8%|â–Š         | 2/25 [00:01<00:13,  1.65it/s, v_num=0, train_loss_step=19.50, train_loss_epoch=19.50]Epoch 1:   8%|â–Š         | 2/25 [00:01<00:13,  1.64it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]Epoch 1:  12%|â–ˆâ–        | 3/25 [00:01<00:11,  1.95it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]Epoch 1:  12%|â–ˆâ–        | 3/25 [00:01<00:11,  1.95it/s, v_num=0, train_loss_step=19.50, train_loss_epoch=19.50]Epoch 1:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:09,  2.17it/s, v_num=0, train_loss_step=19.50, train_loss_epoch=19.50]Epoch 1:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:09,  2.17it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]Epoch 1:  20%|â–ˆâ–ˆ        | 5/25 [00:02<00:08,  2.30it/s, v_num=0, train_loss_step=19.40, train_loss_epoch=19.50]Epoch 1:  20%|â–ˆâ–ˆ        | 5/25 [00:02<00:08,  2.30it/s, v_num=0, train_loss_step=19.50, train_loss_epoch=19.50]Epoch 1:  24%|â–ˆâ–ˆâ–       | 6/25 [00:02<00:07,  2.41it/s, v_num=0, train_loss_step=19.50, train_loss_epoch=19.50]Epoch 1:  24%|â–ˆâ–ˆâ–       | 6/25 [00:02<00:07,  2.41it/s, v_num=0, train_loss_step=19.50, train_loss_epoch=19.50]slurmstepd: error: *** JOB 3216017 ON p2-gpu-1 CANCELLED AT 2025-02-22T15:30:13 ***
